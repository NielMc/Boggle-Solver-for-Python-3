{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scikit-learn - Unit 01 - ML Pipeline and ML tasks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python383jvsc74a57bd0b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f",
      "display_name": "Python 3.8.3 64-bit ('base': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoWVo5csafaS"
      },
      "source": [
        "# Scikit-learn - Unit 01 - ML Pipeline and ML tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fYCxIJfhPwF"
      },
      "source": [
        "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%201%20-%20Lesson%20Learning%20Outcome.png\"> Lesson Learning Outcome"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKAhSxNvhTg0"
      },
      "source": [
        "* **Scikit-learn Lesson is made of 9 units.**\n",
        "* By the end of this lesson, you should be able to:\n",
        "  * Learn and use the workflow for training and to evaluate the ML pipeline\n",
        "  * Create a pipeline according to our dataset and ML task\n",
        "  * Fit Regression, Classification, Cluster, PCA (Principal Component Analysis), and NLP (Natural Language Processing) considering different algorithms\n",
        "  * Learn and use the code to fit in one turn, multiple algorithms with hyperparameters optimization\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ndq4Kg-yhxKm"
      },
      "source": [
        "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%202%20-%20Unit%20Objective.png\"> Unit Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiujW2ORafaX"
      },
      "source": [
        "* Reinforce ML pipeline concepts and the ML tasks that will be covered in the next notebooks.\n",
        "* Learn and use the workflow for training and to evaluate the ML pipeline.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRr9AthgafaZ"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXeFM9FuafaY"
      },
      "source": [
        "Scikit-learn allows you to train machine learning models for classification, regression or clustering. In addition, it provides a wide set of functions for data processing, dimensionality reduction, feature engineering, feature scaling, feature selection, tuning model hyperparameters, creating an ML pipeline, evaluating a models performance and more.\n",
        "\n",
        "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\" https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Question%20mark%20icon.png\n",
        "\">\n",
        " **Why do we study Scikit-learn?**\n",
        "  * Because it is a centralized and complete library for conventional ML, containing a suite of practical modules that helps the data practitioners from development to the deployment of ML pipelines.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfTD1ZiMjE53"
      },
      "source": [
        "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%203%20-%20Additional%20Learning%20Context.png\"> Additional Learning Context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05-LnxO9jGRN"
      },
      "source": [
        "* We encourage you to:\n",
        "  * Add **code cells and try out** other possibilities, ie.: play around with parameters values in a function/method, or consider additional function parameters etc.\n",
        "  * Also, **add your comments** in the cells. It can help you to consolidate the learning. \n",
        "\n",
        "* Parameters in given function/method\n",
        "  * As you may expect, a given function in a package may contain multiple parameters. \n",
        "  * Some of them are mandatory to declare; some have pre-defined values, and some are optional. We will cover the most common parameters used/employed at Data Science for a particular function/method. \n",
        "  * However, you may seek additional in the respective package documentation, where you will find instructions on how to use a given function/method. The studied packages are open source, so this documentation is public.\n",
        "  * **For Scikit learn the link is [here](https://scikit-learn.org/stable/g/). We also will use XGBoost library to train pipelines with eXtreme Gradient Boosting, which is a tree-based algorithm. The documentation is [here](https://xgboost.readthedocs.io/en/latest/index.html)**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-H-A1tntKAwM"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hmB6wo1afaZ"
      },
      "source": [
        "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%204%20-%20Import%20Package%20for%20Learning.png\"> Import Package for Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "si40p3W-pf3u"
      },
      "source": [
        "We will install scikit-learn, xgboost, feature-engine and yellow brick to run our exercises"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbL10VwxphdA"
      },
      "source": [
        "! pip install scikit-learn==0.24.2\n",
        "! pip install xgboost==1.2.1\n",
        "! pip install feature-engine==1.0.2\n",
        "! pip install yellowbrick==1.3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrgSs9DCafaa"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style(\"whitegrid\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxqw3evSafab"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwA-8OVBscpK"
      },
      "source": [
        "## <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Scikit-learn - Unit 01 - ML Pipeline and ML tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufhkhWOYdhkr"
      },
      "source": [
        "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LGMT4nUJbSC"
      },
      "source": [
        "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> In a nutshell, Machine learning is a data-driven approach that uses algorithms to learn patterns and relationships from the data, without being explicitly programmed. \n",
        "* The developer gives the algorithm data and an objective. The algorithm is trained and figures out how to match the objective based on the provided data.\n",
        "* This creates a model, and the trained model is used for predicting behaviours and outputs, allowing decision making on unseen data\n",
        "\n",
        "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> ML is heavily applied in practical terms for multiple use cases in many industries, examples include:\n",
        "* E-mail spam detection\n",
        "* Customer Churn\n",
        "* Text Sentiment Analysis\n",
        "* Fraud Detection\n",
        "* Real-time Ads\n",
        "* Recommendation Engine (ie.: you may watch online movies and after finishing one movie, you receive suggestions on what to watch next)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2qD9lutKAy4"
      },
      "source": [
        "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> We will explore\n",
        "* Pipeline concepts\n",
        "* Data Cleaning and Feature Engineering\n",
        "* Feature Scaling and Feature Selection\n",
        "* ML tasks covered in this lesson\n",
        "* General Workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfrTXVnduz7J"
      },
      "source": [
        "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> **Note**\n",
        "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\">\n",
        "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\">\n",
        "* The overall difficulty perception may escalate in the following upcoming notebooks since we will start using in practical terms a series of concepts we covered in the videos and the previous notebooks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BU8lS7mJjPL"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugLb1iniFj3K"
      },
      "source": [
        "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Pipeline concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIcaHSlSD-5O"
      },
      "source": [
        "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> In the previous lesson, we introduced scikit learn that creates a Pipeline, a sequence of tasks.\n",
        "* In ML, we are interested in arranging a sequence of tasks that are in line with the ML process of **data cleaning, feature engineering, feature scaling, feature selection and model**\n",
        "* In an ML pipeline, the last step is typically the model, and the precedent steps prepare the data for the model\n",
        "\n",
        "We import Pipeline from sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VkvF5r7LCTO"
      },
      "source": [
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvHmatCWLCdO"
      },
      "source": [
        "\n",
        "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> On top of that, the pipeline should identify two outcomes: the training outcome and the prediction outcome. \n",
        "* For that, we use estimators as part of the pipeline steps. There are two types of estimators mainly used: predictors and transformers.\n",
        "  * A predictor estimator, uses methods like **.fit()** and **.predict()**. An ML model uses these methods to learn patterns from the data and is used for predictions afterwards.\n",
        "  * On the other hand, the transformer estimator uses the methods **.fit()** and **.transform()** because it learns from the data and later transforms the data with better distribution. \n",
        "  \n",
        "\n",
        "\n",
        "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> We will demonstrate the differences between fitting models with and without a pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uf5w2A0T9cka"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJFVjq-VFphI"
      },
      "source": [
        "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Data Cleaning and Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPBdDX7GFskC"
      },
      "source": [
        "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> We studied in the feature-engine lesson common techniques to handle data cleaning and feature engineering tasks, using feature-engines built-in transformers or creating your own transformer.\n",
        "* In addition, we arranged this transformer in a pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NO_vpdiS9aLI"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiKwqjOeGD0S"
      },
      "source": [
        "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Feature Scaling and Feature Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K71MmvsrGHqN"
      },
      "source": [
        "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> Once the data is cleaned and engineered, you should consider feature scaling and feature selection. We have studied their definition in the Module: Machine Learning Essentials / Section: ML Pipeline. Please refer to it if you need refreshing.\n",
        "\n",
        "* In this section, we will cover the practical step of feature scaling and feature selection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNjx0kFRGw1j"
      },
      "source": [
        "\n",
        "\n",
        "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Feature Scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2H2VSS04GzWd"
      },
      "source": [
        "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> The scale of a feature is an important aspect when fitting a model. For example, there are algorithms like K-means clustering, Linear and Logistic Regression, Neural Networks that are highly affected by the scale of their features.\n",
        "\n",
        "\n",
        "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> According to Scikit-learn [documentation](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html), feature scaling can be an important preprocessing step for many machine learning algorithms. Standardization involves rescaling the features such that they have the properties of a standard normal distribution with a mean of zero and a standard deviation of one.\n",
        "* The idea behind scaling the features is to make all features within a similar scale.\n",
        "\n",
        "\n",
        "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> We will present `StandardScaler()`, which standardizes the data: it centers the variable at zero. It sets the variance to 1, by subtracting the mean from each observation and dividing by the standard deviation. It is also known as Z score. The documentation is [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
        "* We will cover the StandardScaler transformer in the course as a first go-to option for feature scaling. However, there are other alternatives, and you may check the [documentation](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html) to learn more. \n",
        "\n",
        "\n",
        "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> The tradeoff of feature scaling is that the variable distribution will be slightly different. Still, we will create better conditions for the algorithm to learn the patterns and relationships in the data and generalize on unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znI7fBXZiBvk"
      },
      "source": [
        "Let's use the iris dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqAvBZq_LTVb"
      },
      "source": [
        "df =  sns.load_dataset('iris')\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FswPyfoXiPrQ"
      },
      "source": [
        "We will import `StandardScaler()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPyb5eXNG3Du"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ips5yWSvieAJ"
      },
      "source": [
        "We create a pipeline with a step called 'feature_scaling' and attach `StandardScaler()`. When you don't parse any variables to it, it scales all variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRaoypB8idS9"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "pipeline = Pipeline([\n",
        "      (\"feature_scaling\", StandardScaler()) \n",
        "  ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_WyFwbOi-rX"
      },
      "source": [
        "We will apply this pipeline to the features in the train set. We will learn how to split data soon, but for now, we will manually create a train set and a test set, where each has a set for features and the target variable.\n",
        "* In this dataset, features are `['sepal_length', 'sepal_width', 'petal_length', 'petal_width']` and target is `['species']`. We shuffle the data and will get the first 100 rows and set to the train set. The remaining goes to the test.\n",
        "* <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> There is a proper way to split a train and test set. We will cover that soon.\n",
        "* The central point is to have 2 sets (Train and test) and have features and the target separated.\n",
        "\n",
        "\n",
        "Let's shuffle the data. we use `.sample(frac=1)`, the documentation link is [here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html). It returns a random sample from the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2cZsv8R1iaB"
      },
      "source": [
        "df = df.sample(frac=1)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43lkkINY1iiR"
      },
      "source": [
        "The train set features are X_train, and has the first 100 rows. The train set target is y_train and has the last 50 rows from species. The same rationale goes to the test set, x_test has the first 100 rows and y_test the last 50 rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMt4L0_VjpL6"
      },
      "source": [
        "X_train = df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']][:100]\n",
        "y_train =  df[['species']][:100]\n",
        "X_test =  df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']][100:]\n",
        "y_test =  df[['species']][100:]\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmuHpDZPkuDC"
      },
      "source": [
        "We check the DataFrames dimensions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4lVspC6jpW1"
      },
      "source": [
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-ACFpS1kyDP"
      },
      "source": [
        "When applying pipelines to ML, we fit the pipeline to the train set (so it will learn the parameters) and based on this learning, transform the data on the train and test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3k2UIafjpeF"
      },
      "source": [
        "pipeline.fit(X_train)\n",
        "X_train_scaled = pipeline.transform(X_train)\n",
        "X_test_scaled = pipeline.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNJiCfRWoLfH"
      },
      "source": [
        "One caveat of using sklearn transformers is that they output NumPy arrays, instead of Pandas DataFrames. You may remember that feature-engine outputs DataFrames. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDz1_W5fljem"
      },
      "source": [
        "type(X_train_scaled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tcG_91joYh1"
      },
      "source": [
        "So we need an additional step to convert the scaled data back to a DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWtFUnUKlA-i"
      },
      "source": [
        "X_train_scaled = pd.DataFrame(data= X_train_scaled, columns=X_train.columns)\n",
        "X_test_scaled = pd.DataFrame(data= X_test_scaled, columns=X_train.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gM2IU-KVodYj"
      },
      "source": [
        "Now we are fine to move on, the dataset is a DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJbhu9MelBBN"
      },
      "source": [
        "type(X_train_scaled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3weCLZ15ognd"
      },
      "source": [
        "We are now interested to see the difference in each feature before and after applying StandardScaler().\n",
        "* We create a logic to loop on each feature and plot two histograms in the same plot. One shows the data distribution before applying ``StandardScaler()`` and the other after applying it.\n",
        "* The blue plot is before applying, and the red is after. Note that the red histograms are centred at zero of the x-axis. You will notice the distribution may change a bit, but that is part of the tradeoff we mentioned earlier "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ylbo6qPElubJ"
      },
      "source": [
        "sns.set_style('whitegrid')\n",
        "for col in X_train.columns:\n",
        "  fig, axes = plt.subplots(figsize=(8,5))\n",
        "  sns.histplot(data=X_train, x=col, kde=True, color='b',  ax=axes)\n",
        "  sns.histplot(data=X_train_scaled, x=col, kde=True,color='r', ax=axes)\n",
        "  axes.set_title(f\"{col}\")\n",
        "  axes.legend(labels=['Before Scaling', 'After Scaling'])\n",
        "  plt.show()\n",
        "  print(\"\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNSxWQfVoJCf"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCKjuR6VG3LZ"
      },
      "source": [
        "#### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> Feature Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlFL9TB2G5nq"
      },
      "source": [
        "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> The primary goal of feature selection is to have a process to select the relevant features for fitting an ML model. \n",
        "\n",
        "That is important since: \n",
        "* Models with less and more relevant features are simpler to interpret\n",
        "* You reduce the chance of overfitting by removing features that may add little information or noise.\n",
        "* You reduce the time needed to train the models.\n",
        "* You reduce the feature space. You require less effort from the software development team to design and implement the interface (either API or dashboard) to the production environment.\n",
        "\n",
        "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> This step can be seen as a combination of search techniques to look for a subset of features and an evaluation measure that scores the different feature subsets. There are a few methods for feature selection:\n",
        "* Filter Method\n",
        "* Wrapper Method\n",
        "* Embedded Method\n",
        "\n",
        "\n",
        "\n",
        "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> In the course, as a start point for your career, we will use the Embedded method.\n",
        "* It is named embedded methods since it performs feature selection during the model training. It finds the feature subset for the algorithm that is being trained.\n",
        "* The method automatically trains an ML model, then derives feature importance from it, removing non-relevant features using the derived feature importance.\n",
        "\n",
        "\n",
        "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> For example:\n",
        "* Suppose your pipeline is considering a Decision Tree algorithm in the model step. In that case, you can add before the model step a feature selection step using an embedded method considering a Decision Tree.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnXuPAU6DUy_"
      },
      "source": [
        "Let's reuse the same data from the previous exercise: the iris dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s30cuyZSzJMB"
      },
      "source": [
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
        "\n",
        "X_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGiNS087y46p"
      },
      "source": [
        "\n",
        "We are using `SelectFromModel()` as the method. Its documentation is found [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html). \n",
        "* The argument is the algorithm you are considering in the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzqps_seDRZS"
      },
      "source": [
        "from sklearn.feature_selection import SelectFromModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2j0XMO7DgzJ"
      },
      "source": [
        "We create a pipeline using a Decision Tree algorithm that contains 3 steps:\n",
        "* feature scaling: like we saw in the previous example.\n",
        "* feature selection: use SelectFromModel considering the same algorithm from the model step.\n",
        "* model: uses a Decision Tree algorithm (we will get into more details in upcoming units, for now, take this step as the model step and let's use a decision tree for the example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94IHnEkizJSo"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "pipeline = Pipeline([\n",
        "      ( \"feature_scaling\", StandardScaler() ),\n",
        "      ( \"feature_selection\", SelectFromModel(DecisionTreeClassifier(random_state=101)) ),\n",
        "      ( \"model\", DecisionTreeClassifier(random_state=101) ),\n",
        "  ])\n",
        "\n",
        "pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1Su9iJIEQbI"
      },
      "source": [
        "We fit the pipeline with the Train set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aKTVFukz16P"
      },
      "source": [
        "pipeline.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZPA1OcLETbl"
      },
      "source": [
        "And access the feature_selection step, using bracket notation as we saw in the feature-engine lesson"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeDKucbKz19x"
      },
      "source": [
        "pipeline['feature_selection']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8r9iGAv6EawI"
      },
      "source": [
        "That was not informative. We need to use `.get_support()` to access which features were selected by this step. \n",
        "* The output is a boolean list, where its length and order are related to the original feature space.\n",
        "* For example, the train set has four features. We see that the feature_selection step selected the last two steps since they are True. The first two features were not considered since they are False in the boolean list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LoV84WZz_UB"
      },
      "source": [
        "pipeline['feature_selection'].get_support()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2tZbVLGFlxC"
      },
      "source": [
        "However, we want to know the features list that was selected, not a boolean list.\n",
        "* We then use this boolean list to subset the features.\n",
        "* A quick recap on the features list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AoiSAU50Jg_"
      },
      "source": [
        "X_train.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qx5RDAhpF1W4"
      },
      "source": [
        "We use the boolean list to subset the previous list\n",
        "* And here we have the features that were consideried important for that given dataset using that given algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "razil8480JZB"
      },
      "source": [
        "X_train.columns[pipeline['feature_selection'].get_support()] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5xBX1hZJocw"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kT35ZLmG9St"
      },
      "source": [
        "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> ML tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKG5-2gDHF_z"
      },
      "source": [
        "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> In this lesson, we will explore business cases that involve the following ML tasks\n",
        "* Regression\n",
        "* Classification (Binary and Multi-class)\n",
        "* Clustering\n",
        "* NLP (Natural Language processing)\n",
        "\n",
        "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%205%20-%20Practice.png\"> We will use structured and tabular datasets from ML libraries like Seaborn, Plotly, Sckit-learn and Yellow-brick."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzlsCoTkJpgt"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7tcXsZyHnU6"
      },
      "source": [
        "### <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%2010-%20Lesson%20Content.png\"> General Workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOSHJgRQHu18"
      },
      "source": [
        "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> In a practical project, you can use CRISP-DM workflow to manage project steps. In case you want a refresher on the workflow, revert to the Module Delivering Data Science projects\n",
        "* For this lesson, we will focus on the following CRISP-DM steps: data understanding, data preparation, modelling and evaluation.\n",
        "\n",
        "\n",
        "\n",
        "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%207-%20Note.png\"> Therefore, when you reach the modelling phase in a project, it is assumed you have collected the data, conducted an EDA, and defined the pipeline steps.\n",
        "\n",
        "* When modelling, for supervised learning, you will typically use an overall workflow like:\n",
        "  * Split the dataset into train and test set\n",
        "  * Fit the model (either a pipeline or not) \n",
        "  * Evaluate your model. If performance is not good, revisit the process, starting from collecting the data, conducting EDA etc\n",
        "\n",
        "\n",
        "There are some potential small variations to this workflow, but this is the starting point we consider in your journey of modelling\n",
        "\n",
        "\n",
        "<img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> **HUGE WARNING** <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\"> <img width=\"3%\" height=\"3%\" align=\"top\"  src=\"https://codeinstitute.s3.amazonaws.com/predictive_analytics/jupyter_notebook_icons/Icon%206%20-%20Warning.png\">\n",
        "* **Reflect** for a second how many steps and considerations you need before fitting a model. You will be surprised that in a project where a person is responsible from end to end, the modelling phase will take a small percentage of your time and attention.\n",
        "\n",
        "* Even though, this phase it critical to your project, **so let's stop the reading/talking and let's fit some models**.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOaYIdqeJqpo"
      },
      "source": [
        "---"
      ]
    }
  ]
}